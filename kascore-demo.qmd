---
title: "{KAscore}"
subtitle: | 
  | The Breakthrough R Package for Scorecard Models
  | from Ketchbrook Analytics
format: 
  revealjs:
    theme: default
    chalkboard: true
    logo: www/ketchbrook_logo.png
    incremental: false
    transition: slide
    preview-links: true
---

## What is {KAscore}?

:::: {.columns}

::: {.column width="50%"}
- R package that contains a suite of functions for developing & managing credit scorecard models
- [{KAscore} Website](https://ketchbrookanalytics.github.io/KAscore)
:::

::: {.column width="50%"}
<img src="www/intelligent_credit_scoring_book_cover.jpg" align="right" height="500"/>
:::

::::

## Data Science in the Farm Credit System

::: {.incremental}
- Modern tools call for modern solutions
- ~~Platform~~ Software as a Service
- Let's build together
:::

::: {.notes}
- When I started at Farm Credit East back in 2017, the most cutting-edge analytics software available in the Farm Credit System were these giant Macro-enabled Excel workbooks
+ and these Excel tools actually worked well for a lot of tasks that folks were doing years ago, mostly just descriptive analytics, and the only deliverables the analytics team was responsible for were static reports on a quarterly or annual basis
+ but the analytics function within Farm Credit has come a *long* way since then
+ folks are building credit risk models, some of which actually end up as part of a production process within the ACA, including scorecards
+ open source software (particular the R language) has evolved as the tool-of-choice for most analytics teams within the System for a number of reasons
- ability to easily handle a lot more data than Excel
- reproducibility (I like to think of code as a "recipe", and taking a code-first approach provides you with a "recipe" for your analysis; got new data in this month?  Just apply the same "recipe" pointed at the new data)
- the ecosystem of packages available for R is continuing to grow; from packages that just make it easier for you to transform and manipulate data to packages that are super-specific and -- for example -- help you easily develop a credit scorecard model; if you are working on writing R code to solve some specific use case, chances are that there's an R package out there that will help
+ And there are really two types of vendors out there that we see:
1. Vendors who offer these Excel workbooks to help Excel users with their work, and 
2. Vendors who are offering proprietary software that becomes another *platform* that your team has to manage
- but there's nobody really meeting data analysts where they are -- in the open source languages they work in everyday (R, Python, and SQL) -- there's no vendor helping them improve their quality of life and workflows within the tools they already use and are comfortable with -- that's the need that Ketchbrook is trying to serve
- With that in mind, Ketchbrook Analytics is committed to *building* software that meets you where you are (R packages, Python packages for those using Python, Shiny web applications); software that allows you to do your job easier and lets you use the tooling that actually fits into your current workflow
- At Ketchbrook, we have a really strong consulting practice that includes model development & validation services, among a bunch of other data science offerings
+ But we know that you have really brilliant analytics staff working within your ACAs that you have hired over the past few years, and you don't need us to do *everything*
+ Additionally, a lot of the problems you are up against are problems that you've never encountered before at your ACA -- CECL is a brand new initiative, what's expected for stress testing today is *way* different than what we used to be able to get away with; so collaboration is KEY
+ We want to be part of that collaboration; we believe that we can be a valuable partner in building the software that's going to solve these problems
- One of my calls to action today is: "Let's build together"
+ We want to hear from *you* about what we should build next (Should it be an entire CECL framework as an R package? What is going to make your life easier?)
+ This R package we are demo-ing today is hopefully a really great example of a need that existed (and continues to exist) in the System where we collaborated with a few of the ACAs who are on the call today to build software that is useful to you, in a way that is extensible to fit the nuances of however you need to implement or monitor your own credit scorecards
:::

## Demo

```{.r}
library(KAscore)

# Check out the built-in `loans` dataset

loans
```

<br>

```{r}
library(KAscore)

# Reverse levels in dependent variable
loans$default_status <- factor(
  loans$default_status, 
  levels = c("good", "bad")
)

# Pass first 5 rows of data to OJS
ojs_define(loans_ojs = head(loans, n = 5L))
```

```{ojs}
Inputs.table(transpose(loans_ojs))
```

::: {.notes}
- One thing that anyone who knows me well knows is that I really dislike slide deck presentations... We just want to build something that fits your use case and works really well and then show you how it works -- so that's exactly what we're going to do next
- Once you've installed {KAscore}, you can load it just like any other R package
- One fun thing about the package is that it contains a dataset called `loans` that allows you to get started playing around with the package's functions without even needing to bring in your own data
- Each observation in this table represents a unique loan, and notice that we have a bunch of loan attributes (which will serve as potential independent variables) and the `default_status` (which will serve as the dependent variable)
- The actual dataset contains 1,000 rows, but we'll just show the first few today for demo purposes
:::

## Weight of Evidence

```{.r code-line-numbers="4|5|6|7"}
# Calculate the Weight-of-Evidence values for the
# "collateral_type" and "housing_status" variables

woe(
  data = loans,
  outcome = default_status,
  predictors = c(collateral_type, housing_status)
)
```

<br>

```{r}
ojs_define(
  woe_ojs = woe(
    data = loans,
    outcome = default_status,
    predictors = c(collateral_type, housing_status), 
    verbose = FALSE
  ) |> 
    dplyr::select(-(tidyselect::starts_with("p_")))
)
```

::: {.fragment}
```{ojs}
Inputs.table(transpose(woe_ojs))
```
:::

::: {.notes}
- I promise we are going to get to showing you how to build a scorecard from start-to-finish, but in order to do that, we need to first set it up with a walkthrough of some of the specific functions & calculations involved
- If you've never been involved in building a scorecard before, you might not be familiar with the concept of "Weight of Evidence"
- This is a feature engineering technique that actually *needs* to take place prior to building your scorecard
- Instead of leaving your categorical predictor variables as they are, we transform them to their Weight-of-Evidence equivalents by calculating the distribution of "goods" in each class divided by the distribution of "bads" in that class (when I say "goods" and "bads", I'm referring to the values in `default_status`, our dependent variable)
- {KAscore} has a function called `woe()` that can calculate these WoE values for us
  + `woe()`'s first argument is the data frame containing the independent & dependent variables; note that having the `data` as the first argument makes the function very pipe-friendly for those using {magrittr} or the new base pipe
  + the second argument `outcome` asks you to specify the dependent variable in the data frame
  + and the `predictors` argument lets you list the different independent variables for which you want to calculate WoE values 
  + by default, our `woe()` function boils down the dataset into just the unique classes across the independent variables, and returns a "dictionary" of the unique Weight-of-Evidence values
- It's pretty easy to interpret WoE values:
  + positive (and higher) WoE values indicate that there is a larger proportion of "goods" than "bads" in that class
  + negative (and lower) WoE values indicate that there is a larger proportion of "bads" than "goods" in that independent variable class
:::

## Weight of Evidence

```{.r code-line-numbers="8"}
# Instead of creating a "dictionary" of the unique WoE values,
# add the WoE values to the original data frame

woe(
  data = loans,
  outcome = default_status,
  predictors = c(collateral_type, housing_status),
  method = "add"
)
```

<br>

```{r}
ojs_define(
  woe_add_ojs = woe(
    data = loans,
    outcome = default_status,
    predictors = c(collateral_type, housing_status), 
    method = "add",
    verbose = FALSE
  ) |> 
    dplyr::slice(3L:8L) |> 
    dplyr::bind_cols(
      loans[3L:8L, "loan_id"]
    ) |> 
    dplyr::relocate(loan_id, .before = tidyselect::everything())
)
```

```{ojs}
Inputs.table(transpose(woe_add_ojs))
```

::: {.notes}
- We could instead specify `method = "add"`, and `woe()` would preserve the structure of the original data frame and append the Weight-of-Evidence values as new columns
:::

## Weight of Evidence

```{.r code-line-numbers="8"}
# Or we can replace the original independent variables with
# their WoE equivalents, via `method = "replace"`

woe(
  data = loans,
  outcome = default_status,
  predictors = c(collateral_type, housing_status),
  method = "replace"
)
```

<br>

```{r}
ojs_define(
  woe_replace_ojs = woe(
    data = loans,
    outcome = default_status,
    predictors = c(collateral_type, housing_status), 
    method = "replace",
    verbose = FALSE
  ) |> 
    dplyr::slice(3L:8L) |> 
    dplyr::bind_cols(
      loans[3L:8L, "loan_id"]
    ) |> 
    dplyr::relocate(loan_id, .before = tidyselect::everything())
)
```

```{ojs}
Inputs.table(transpose(woe_replace_ojs))
```

::: {.notes}
- Or, if we specify `method = "replace"`, we can simply replace the independent variables we specified with their WoE equivalents
:::

## Information Value

```{.r code-line-numbers="1|2|3|4-10"}
iv(
    data = loans,
    outcome = default_status,
    predictors = c(
      amount_of_existing_debt,
      collateral_type,
      housing_status,
      industry,
      years_at_current_address
    )
)
```

<br>

```{r}
ojs_define(
  iv_ojs = iv(
    data = loans,
    outcome = default_status,
    predictors = c(
      amount_of_existing_debt,
      collateral_type,
      housing_status,
      industry,
      years_at_current_address
    ),
    verbose = FALSE, 
    labels = TRUE
  )
)
```

::: {.fragment}
```{ojs}
Inputs.table(transpose(iv_ojs))
```
:::

::: {.notes}
- The next function I want to demonstrate is called `iv()`, which is short for "information value"
- Information Value is a useful by-product of Weight-of-Evidence transformation, and it can provide insight into the relative feature importance of each WOE-transformed independent variable towards your scorecard model
- The arguments to `iv()` are very similar to the last function
  + we have to define our data, outcome, and which independent variables in the data we want to calculate the information value of (in this case, we'll throw a bunch of the columns from our dataset at it)
- The `iv()` function is going to return to you not only the information value statistic for each variable, but also the plain-English interpretation of each independent variables predictiveness
  + This is all based upon the literature in Siddiqi's book
  + You may want to consider omitting any variables from your model that have an information value indicating they are "Likely overfit" or "Not predictive"
  + Even better, you should be using information value in your model monitoring processes to track any changes to the predictiveness of the independent variables in your scorecard models -- clearly {KAscore} can easily help you do that
:::

## Build a Scorecard

```{.r code-line-numbers="3|4|5|6|7"}
# Create our training data

train <- loans |> 
  woe(
    outcome = default_status,
    predictors = c(collateral_type, housing_status, industry),
    method = "replace"
  )
```

<br>

```{r}
train <- loans |> 
  woe(
    outcome = default_status,
    predictors = c(
      collateral_type,
      housing_status,
      industry
    ),
    method = "replace",
    verbose = FALSE
  )

ojs_define(
  train_ojs = head(train, n = 50L) |> 
    dplyr::bind_cols(
      loans[1L:50L, "loan_id"]
    ) |> 
    dplyr::relocate(loan_id, .before = tidyselect::everything())
)
```

```{ojs}
Inputs.table(transpose(train_ojs))
```

::: {.notes}
- So let's actually build a scorecard model from start to finish!
- As I said from the outset, the first thing we need is our dependent variable and whichever weight-of-evidence transformed indpendent variables we want to include in our model
- If we look back at our information value output, it looks like "collateral type", "housing status" and "industry" had some level of predictiveness that wouldn't overfit the model
- So we can pass those three columns to our `woe()` function, and by setting `method = "replace"`, we can create a model training dataset that's ready to be passed to an algorithm
:::

## Build a Scorecard

```{r}
#| echo: true

# Fit the logistic regression model

fit <- glm(
  formula = default_status ~  ., 
  data = train, 
  family = "binomial"
)
```

<br>

```{r}
params <- broom::tidy(fit) |> 
  dplyr::mutate(
    signif = dplyr::case_when(
      p.value < 0.001 ~ "***",
      p.value >= 0.001 & p.value < 0.01 ~ "**",
      p.value >= 0.01 & p.value < 0.05 ~ "*", 
      TRUE ~ ""
    )
  ) |> 
  as.data.frame()

params
```

::: {.notes}
- Logistic regression is really the de facto algorithm that is used across all scorecard models, because the its interpretability of its output as odds ratios; and we'll see shortly how *odds* is a major concept in bridging the gap between our logistic regression model and what we really are after -- a points-based scorecard
- We can take a look at some of the summary statistics around our model
  + One thing that might be interesting is that all of the coefficients are negative
  + This might seem strange at first, until you realize two things:
  + First, this logistic regression model is looking at our dependent variable `default_status` and its going to output a probability of a loan being *bad* (not a probability of a *good*) -- this is typical when building credit risk models, we want a model that outputs a probability of default (obviously we can just subract the probability from 1 to get the probability of *good*)
  + With that being said, remember that for the actual WoE values, positive values mean that there are more *goods* than *bads* in that class; so as you have a higher WoE value, when it gets multiplied by the coefficient we are looking at here, that's going to lead to a lower model output value, or a lower probability of *bad*
  + So, if you have generally a lot more *goods* than *bads* in your training data, you should pretty much always be seeing negative coefficients in your logistic regression model
- Long story short, everything looks good here
:::

## Target Points/Odds {auto-animate=true}

```{r}
#| echo: true

# A loan that scores 600 points has 30:1 odds of being "good"

# I.e., loans that score 600 points have a 97% probability of being "good",
# and a 3% probability of being "bad"

target_points <- 600
target_odds <- 30
```

::: {.notes}
- There are a few managerial decisions that need to be made when developing a scorecard, and they're all related to converting that logistic regression model to an eventual number of points; these decisions lay the roadmap for how we are going to do that
- The first two things we need to specify are the baseline points & odds values, which we need to determine at the same time
- E.g., we can say that a score of 600 points would have 30:1 odds of being "good"
  + said another way, for every 31 loans that score 600 points, we would expect 30 of them to turn out to be *good* (or pay back in full) and 1 of them to turn out to be *bad* (and default, for example)
:::

## Target Points/Odds {auto-animate=true}

```{r}
#| echo: true
#| code-line-numbers: "9-12"

# A loan that scores 600 points has 30:1 odds of being "good"

# I.e., loans that score 600 points have a 97% probability of being "good",
# and a 3% probability of being "bad"

target_points <- 600
target_odds <- 30

# Every 50 points (+/-), the odds double (or halve):

growth_points <- 50
growth_rate <- 2
```

::: {.notes}
- Once we have our baseline established, we need to determine how the odds change as the number of points we assign changes
- In this case, we can specify that every 50 points, the odds would double
  + So at a score of 650, we would expect odds of 60:1 (instead of 30:1)
  + Conversely, at a score of 550 (50 points the other way), we would expect odds of 15:1
- If we wanted the odds to triple every 50 points, we could set our `growth_rate` to be 3 instead of 2
:::

## Target Points/Odds {auto-animate=true}

```{r}
#| echo: true
#| code-line-numbers: "14-16"

# A loan that scores 600 points has 30:1 odds of being "good"

# I.e., loans that score 600 points have a 97% probability of being "good",
# and a 3% probability of being "bad"

target_points <- 600
target_odds <- 30

# Every 50 points (+/-), the odds double (or halve):

growth_points <- 50
growth_rate <- 2

# Let's simulate a bunch of scores from 500 to 700 (by 25)

scores <- seq.int(from = 500, to = 700, by = 25)
```

::: {.fragment}
```{r}
scores
```
:::

::: {.notes}
- It's a lot easier to visualize this graphically, so let's simulate a bunch of scores from 500 to 700 and take a look at the associated odds at each score and how the odds change exponentially, instead of linearly
:::

## Target Points/Odds {auto-animate=true}

```{.r code-line-numbers="1|2|3|4|5|6"}
odds(
  score = scores, 
  tgt_points = target_points,   # 600
  tgt_odds = target_odds,   # 30
  pxo = growth_points,   # 50
  rate = growth_rate   # 2
)
```

<br>

```{r}
odds_by_score <- odds(
  score = scores, 
  tgt_points = target_points, 
  tgt_odds = target_odds, 
  pxo = growth_points, 
  rate = growth_rate
)

odds_tbl <- tibble::tibble(
    score = scores,
    odds = odds_by_score |> round(2)
  )

ojs_define(
  odds_ojs = odds_tbl
)
```

::: {.fragment}
```{ojs}
Inputs.table(transpose(odds_ojs))
```
:::

::: {.notes}
- Conveniently, {KAscore} has a function called `odds()` that will calculate these associated odds for you, given a vector of scores (which we just created), your baseline points & odds (which we specified earlier as `target_points` and `target_odds`), and then our scaling facotrs we defined in `growth_points` and `growth_rate`
- This function is going to return the associated odds at each score that we passed it
- It's probably worth plotting the data in this table though to understand how the relationship changes as the score increases/decreases  
:::

## Target Points/Odds {.nostretch transition="none-out"}

```{r}

p <- odds_tbl |> 
    ggplot2::ggplot(
        ggplot2::aes(
            x = scores,
            y = odds
        )
    ) + 
    ggplot2::geom_point() + 
    ggplot2::geom_line() + 
    ggplot2::labs(
        x = "Score",
        y = "Odds (bad:good, reduced to y:1)"
    ) +
    ggplot2::scale_y_continuous(
        breaks = seq(0, 140, 20),
        expand = c(0, 0), 
        limits = c(0, 125)
    ) + 
    ggplot2::theme_minimal()

p
```

::: {.notes}
- Here's that same data, just plotted on a chart with the score on the x-axis and the associated odds on the y-axis
- We can clearly see that the relationship is not linear, its exponential
:::

## Target Points/Odds {.nostretch transition="none-in"}

```{r}

p + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 500,
      xend = 600,
      y = 30, 
      yend = 30
    ), 
    linetype = "dashed",
    color = "red"
  ) + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 600,
      xend = 600,
      y = 0, 
      yend = 30
    ), 
    linetype = "dashed",
    color = "red"
    )
```

::: {.notes}
- As we specified, a score of 600 has 30:1 odds
:::

## Target Points/Odds {.nostretch transition="none-in"}

```{r}

p + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 500,
      xend = 650,
      y = 60, 
      yend = 60
    ), 
    linetype = "dashed",
    color = "red"
  ) + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 650,
      xend = 650,
      y = 0, 
      yend = 60
    ), 
    linetype = "dashed",
    color = "red"
    )
```

::: {.notes}
- A score of 650 has 60:1 odds (for every 61 loans that score a 650, we expect 60 to be *good* and 1 to be *bad*)
:::

## Target Points/Odds {.nostretch transition="none-in"}

```{r}

p + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 500,
      xend = 700,
      y = 120, 
      yend = 120
    ), 
    linetype = "dashed",
    color = "red"
  ) + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 700,
      xend = 700,
      y = 0, 
      yend = 120
    ), 
    linetype = "dashed",
    color = "red"
    )
```

::: {.notes}
- A score of 700 has 120:1 odds
:::

## Target Points/Odds {.nostretch transition="none-in"}

```{r}

p + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 500,
      xend = 550,
      y = 15, 
      yend = 15
    ), 
    linetype = "dashed",
    color = "red"
  ) + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = 550,
      xend = 550,
      y = 0, 
      yend = 15
    ), 
    linetype = "dashed",
    color = "red"
    )
```

::: {.notes}
- And a score of 550 has 15:1 odds
- So we think that `odds()` function is really convenient for showcasing the relationship between scores and odds, enabling you to have those conversations with management about your scorecard before you have to decide on the number of target points, odds, and scaling factors you want to use in your own scorecard
:::

## Calculate the Points {auto-animate=true}

1. Weight-of-Evidence values for each class in each independent variable
2. Model intercept and coefficients for each independent variable
3. Target points/odds and scaling quantity/rate

::: {.notes}
- Now, to actually build *our* scorecard, we are going to need all of the things we just talked about <read slide>
:::

## Calculate the Points {auto-animate=true transition="none-out"}

1. Weight-of-Evidence values for each class in each independent variable

```{r}
# Build the scorecard base dictionary
dict <- woe(
  data = loans,
  outcome = default_status,
  predictors = c(collateral_type, housing_status, industry),
  method = "dict",
  verbose = FALSE
) |>
  dplyr::select(variable, class, woe)

ojs_define(card_woe_ojs = dict)
```

```{ojs}
Inputs.table(transpose(card_woe_ojs))
```

::: {.notes}
- Let's start with our weight-of-evidence dictionary
:::

## Calculate the Points {transition="none-in"}

2. Model intercept and coefficients for each independent variable

```{r}
# Extract the model's parameter estimates & intercept
params <- fit$coefficients |>
  tibble::as_tibble(rownames = NA) |>
  tibble::rownames_to_column(var = "variable") |> 
  dplyr::mutate(variable = stringr::str_replace(variable, "woe_", ""))

dict <- dict |> 
  dplyr::inner_join(params, by = "variable")

ojs_define(card_coefs_ojs = dict |> dplyr::rename(coef = value))
```

```{ojs}
Inputs.table(transpose(card_coefs_ojs))
```

::: {.notes}
- Then we join in the model coefficients for each variable in the table
:::

## Calculate the Points {auto-animate=true}

3. Target points/odds and scaling quantity/rate

```{.r code-line-numbers="1-5|6|7|8|9|10-13"}
# Add the points, creating the final scorecard

dict |>
  dplyr::mutate(
    points = points(
      woe = woe,
      estimate = coef,
      intercept = params$value[params$variable == "(Intercept)"],
      num_vars = length(params$variable[params$variable != "(Intercept)"]),
      tgt_points = 600,
      tgt_odds = 30,
      pxo = 50,
      rate = 2
    )
  )
```

::: {.notes}
- Lastly, we can use the `points()` function from {KAscore} to calculate the number of points each class gets; `points()` takes the following arguments
- the column containing the WoE values
- the column containing the model coefficients
- the model intercept, which we can lookup from our model summary statistics table
- the number of independent variables in our model, which we can also lookup from our model summary statistics table
- the baseline target number of points we want for our scorecard
- the baseline odds that the target points get
- and the number of points (50) to double the odds
:::

## Calculate the Points {auto-animate=true transition="none-out"}

3. Target points/odds and scaling quantity/rate

```{r}
# Add the points, creating the final scorecard
card <- dict |>
  dplyr::mutate(
    points = points(
      woe = woe,
      estimate = value,
      intercept = params$value[params$variable == "(Intercept)"],
      num_vars = length(params$variable[params$variable != "(Intercept)"]),
      tgt_points = 600L,
      tgt_odds = 30L,
      pxo = 50L,
      rate = 2L
    )
  )

ojs_define(card_points_ojs = card |> dplyr::rename(coef = value))
```

```{ojs}
Inputs.table(transpose(card_points_ojs))
```

::: {.notes}
- And that's it!  Now we have the number of points for each class of each independent variable!
- And when we go to hand this off to others, we can probably strip away the `woe` and `coef` columns so that we just have... <next slide>
:::

## Calculate the Points {transition="none-in"}

3. Target points/odds and scaling quantity/rate

```{r}
ojs_define(
  card_points_ojs_2 = card |> 
    dplyr::select(variable, class, points)
)
```

```{ojs}
Inputs.table(transpose(card_points_ojs_2))
```

## Score a New Loan Application {transition="none-out"}

Imagine a new loan applicant comes through the door with the following characteristics:

:::: {.columns}

::: {.column width="65%"}
- Collateral Type: *Real Estate*

- Housing Status: *Own*

- Industry: *Poultry*
:::

::: {.column width="35%"}
```{r}
new_app <- tibble::tribble(
  ~variable, ~class,
  "collateral_type", "real estate",
  "housing_status", "own",
  "industry", "poultry"
)

ojs_define(new_app_ojs = new_app)
```

::: {.fragment}
```{ojs}
Inputs.table(transpose(new_app_ojs))
```
:::

:::

::::

::: {.notes}
- We can put this information into a table
:::

## Score a New Loan Application {transition="none-in none-out"}

Imagine a new loan applicant comes through the door with the following characteristics:

:::: {.columns}

::: {.column width="65%"}
- Collateral Type: *Real Estate*

- Housing Status: *Own*

- Industry: *Poultry*
:::

::: {.column width="35%"}
```{r}
new_app_points <- new_app |> 
  dplyr::inner_join(
    card |> 
      dplyr::select(variable, class, points), 
    by = c("variable", "class")
  )

ojs_define(new_app_points_ojs = new_app_points)
```

```{ojs}
Inputs.table(transpose(new_app_points_ojs))
```
:::

::::

::: {.notes}
- And then, using our scorecard, we can lookup the number of points they get for each attribute
:::

## Score a New Loan Application {transition="none-in"}

Imagine a new loan applicant comes through the door with the following characteristics:

:::: {.columns}

::: {.column width="65%"}
- Collateral Type: *Real Estate*

- Housing Status: *Own*

- Industry: *Poultry*
:::

::: {.column width="35%"}
```{r}
new_app_total <- new_app_points |> 
  dplyr::bind_rows(
    tibble::tibble(
      variable = "TOTAL",
      class = "",
      points = sum(new_app_points$points)
    )
  )

ojs_define(new_app_total_ojs = new_app_total)
```

```{ojs}
Inputs.table(transpose(new_app_total_ojs))
```
:::

::::

::: {.notes}
- If we sum it all up, we can see that this new applicant's total scorecard score would be 505
:::

## Score a New Loan Application {transition="none-out"}

```{r}
#| echo: true
#| code-line-numbers: 1|2|3-6

applicant_odds <- odds(
  score = 505, 
  tgt_points = 600, 
  tgt_odds = 30, 
  pxo = 50, 
  rate = 2
)
```

::: {.notes}
- From a quantitative perspective we might be interested in seeing what this applicant's odds of being a *good* loan candidate are; we can go back to our odds function, plug in their score to the `score` argument, and pass the baseline criteria to the other arguments
:::

## Score a New Loan Application {transition="none-in none-out"}

```{r}
#| echo: true
#| code-line-numbers: 9

applicant_odds <- odds(
  score = 505, 
  tgt_points = 600, 
  tgt_odds = 30, 
  pxo = 50, 
  rate = 2
)

paste0("Applicant's Odds: ", round(applicant_odds, 2), ":1") |> print()
```

::: {.notes}
- This applicant has a little over 8:1 odds of being a *good* loan
:::

## Score a New Loan Application {transition="none-in"}

```{r}
#| echo: true
#| code-line-numbers: 9-10

applicant_odds <- odds(
  score = 505, 
  tgt_points = 600, 
  tgt_odds = 30, 
  pxo = 50, 
  rate = 2
)

prob_good <- applicant_odds / (applicant_odds + 1)
prob_bad <- 1 - prob_good

paste0(
  "Applicant's Probability of *Good*: ", round(prob_good, 3) * 100, "%\n",
  "Applicant's Probability of *Bad*: ", round(prob_bad, 3) * 100, "%"
) |> cat()
```

::: {.notes}
- I'm not a huge fan of reporting odds, so if we prefer to express that as a probability, then its a pretty easy calculation to tell us that 8:1 odds is about 11% probability of being *bad* (and obviously then an 89% probability of being *good*)
:::

## Using {KAscore} to Validate PD Rating "Models"

Scorecards & PD Rating Assignment Models

::: {.fragment}
![](www/not_so_different.gif){width="800"}
:::

::: {.notes}
- One interesting thing is how prevelant card-based "models" are in the Farm Credit System; and once we built this package and ACAs started using it, we realized that there are a lot more uses for its functionality than we initially even anticipated
- One of those uses is for the purposes of validating PD Rating assignment "models" (and we'll use "models" liberally here, because most of these are not quantitatively defined)
:::

## Example PD Rating Model {.smaller}

```{r}
tibble::tribble(
  ~Current_Ratio_Bin, ~Rating,
  "[4.0, Inf)", 4L,
  "[3.5, 4.0)", 5L,
  "[3.0, 3.5)", 6L,
  "[2.5, 3.0)", 7L,
  "[2.0, 2.5)", 8L,
  "[1.5, 2.0)", 9L,
  "[1.0, 1.5)", 10L,
  "[0.5, 1.0)", 11L,
  "[0.0, 0.5)", 12L,
  "[-0.5, 0.0)", 13L,
  "[-0.5, -Inf)", 14L,
) |> 
  knitr::kable()
```

::: {.notes}
- And we know that a PD Rating models is simply a lookup table, like the one above for current ratio
- If the applicant's current ratio is greater than 4.0, they get a 4 PD Rating, if it's between 3.5 and 4.0, they get a 5 PD Rating, and so on (typically up to 14)
:::

## Unique Aspects of PD Rating Models

::: {.incremental}
1. PD Ratings (1-14) introduce an *ordinal* variable, which are used as an attempt to rank-order probability of default (which is a *continuous* variable)
2. Any continuous independent variables (e.g., Current Ratio) become binned, categorical variables representing unique intervals
:::

## Validating PD Rating Models

How do we know that our bin breaks are correct/good?

::: {.fragment}
**{KAscore} can help**
:::

::: {.incremental}
- Use `bin_quantile()` to easily bin continuous variables
- Use `iv()` to evaluate predictiveness
:::

## Binning with `bin_quantile()` {auto-animate=true}

Supposed we have the following raw data (a data frame called `binning_data`):

<br>

```{r}
set.seed(1234)
binning_data <- KAscore::loans |> 
  dplyr::mutate(
    default_status = factor(default_status, levels = c("good", "bad")), 
    current_ratio = dplyr::case_when(
      
      amount_of_existing_debt == levels(KAscore::loans$amount_of_existing_debt)[1] ~ 
        sample(seq(2.5, 5.5, 0.1), size = nrow(KAscore::loans), replace = TRUE),
      
      amount_of_existing_debt == levels(KAscore::loans$amount_of_existing_debt)[2] ~
        sample(seq(1.5, 3.5, 0.1), size = nrow(KAscore::loans), replace = TRUE),

      amount_of_existing_debt == levels(KAscore::loans$amount_of_existing_debt)[3] ~
        sample(seq(0.5, 2.5, 0.1), size = nrow(KAscore::loans), replace = TRUE),
      
      TRUE ~ sample(seq(-0.5, 1.2, 0.1), size = nrow(KAscore::loans), replace = TRUE)
    )
  ) |> 
  dplyr::select(loan_id, current_ratio, default_status)

ojs_define(binning_data_ojs = head(binning_data, n = 50L))
```


```{ojs}
Inputs.table(transpose(binning_data_ojs))
```

## Binning with `bin_quantile()` {auto-animate=true}

We can convert the continuous variable `current_ratio` to its binned, categorical equivalent using `bin_quantile()`

:::: {.columns}

::: {.column width="59%"}
```{r}
#| echo: true
#| code-line-numbers: 3|4|5|6|10

binned_data <- binning_data |> 
  dplyr::mutate(
    current_ratio_binned = bin_quantile(
      x = binning_data$current_ratio, 
      n_bins = 11,
      decimals = 2
    ))
```
:::

```{r}
ojs_define(
  binned_data_ojs = head(binned_data, n = 6L) |> dplyr::select(-default_status)
)
```

::: {.column width="41%"}

::: {.fragment}
```{ojs}
Inputs.table(transpose(binned_data_ojs))
```
:::

:::

::::

<br>

::: {.fragment}
```{r}
#| echo: true

# Show the unique bin breaks
levels(binned_data$current_ratio_binned)
```
:::

## Binning with `bin_quantile()` {auto-animate=true}

We can think of this as a PD Card:

```{r}
#| echo: true

# Show the number of observations in each bin

data.frame(
  Bin = levels(binned_data$current_ratio_binned),
  PD_Rating = 14:4
)
```

## Binning with `bin_quantile()` {auto-animate=true}

We can then use `iv()` to evaluate this binned variable's predictiveness:

```{r}
#| message: false
#| echo: true

iv(
  binned_data,
  outcome = default_status,
  predictors = current_ratio_binned, 
  labels = TRUE
)
```

::: {.notes}
We actually developed a Shiny web application that allows you to create bins in a point-and-click way that calls `bin_quantile()` under the hood but allows you to manually override the bins and evaluate how the information value statistic changes -- it's a great way to test out a bunch of different possible bin breaks and see which one is the strongest in terms of predictiveness. Let's check it out!
:::

## Roadmap

- Enhancements & Community Calls
- Short-Term
  + Monitoring functions
  + Deployment examples (Docker)
  + Auto-binning algorithm
- Long-Term
  + Functionality/documentation for other types of models
  + Community-driven enhancements

::: {.notes}
- Those of you on the call who already license the package know that we provide a minimum of four releases of {KAscore} each year, which include bug fixes, enhancements to functionality, and improvements to documentation
- In terms of what's to come...
  + we want to add functionality to make it easy to monitor your scorecard models
  + we will be adding documentation & examples for deploying scorecards into a production environment with Docker & Azure (or any other cloud provider if there are ACAs on this call using AWS or Google Cloud)
  + provide an algorithm for automatically binning continuous variables into the most optimal bins (which is tricky because it includes determining the optimal *number* of bins, *and* the optimal cutoff points) 
- I want to highlight one thing though... it doesn't make sense for us to incorporate functions that already exist into {KAscore}... I'm thinking of calculating model accuracy statistics for monitoring purposes -- the {yardstick} package already does an amazing job of doing this in just a few lines of code -- there's no real way that we could make it better, and we don't want to add a million dependencies to {KAscore} that would make it take 10 minutes to install
  + what we *will* do though, is provide documentation in the form of articles and code that show you exactly how to do these things, even if they incorporate packages other than {KAscore}
- In the longer term, we want to add the ability to use algorithms other than logistic regression for creating scorecards; it's an area where there's not a whole lot of research or scholarly papers available, so we want to take the time to do that research correctly and ensure that we are able to continue to output statistically robust scorecards using more modern binary classification algorithms like XGBoost or Random Forests
- Lastly, we want these enhancements to be driven by the users of the {KAscore} package -- you really have to be hands-on using the package in order to have those "aha!" moments and say, "It would be really useful to have *this* feature or *that* feature" 
:::

## Feedback / Q&A / Contact

<br>

[ketchbrook.com](https://www.ketchbrookanalytics.com)

<br>

[info@ketchbrookanalytics.com](mailto:info@ketchbrookanalytics.com?subject=KAscore)
